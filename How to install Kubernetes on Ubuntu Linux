# Important:

# Linux kernel must be 
# Kernel level: In modern distros (Ubuntu 22.04+, Debian 12+, RHEL 9+),

# Disable swap on each vm where k8s cluster is required to be installed

sudo swapoff -a         



# Can a kubernetes master node and worker node be installed on the same linux vm?
<!-- Question:
Answer:
On a single Linux VM, both master and worker roles can coexist:

The control plane runs normally.

The node also registers itself as a worker.

This is what minikube start, kind create cluster, or kubeadm init + kubectl taint nodes --all node-role.kubernetes.io/control-plane- do.

Example with kubeadm:

# Initialize control plane
sudo kubeadm init --pod-network-cidr=10.244.0.0/16

# Configure kubectl
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Allow master to run workloads
kubectl taint nodes --all node-role.kubernetes.io/control-plane-


Now your master node is also a worker, so pods can schedule there. -->

# Enable IPv4 packet forwarding
# To manually enable IPv4 packet forwarding:

# sysctl params required by setup, params persist across reboots

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system
Verify that net.ipv4.ip_forward is set to 1 with:

sysctl net.ipv4.ip_forward

# Install containerd

sudo apt-get update
sudo apt-get install containerd

# Verify the containerd installation succeeded

ubuntu@devops-02:~$ containerd --version
containerd github.com/containerd/containerd 1.7.27


sudo containerd config default | sed 's/SystemdCgroup = false/SystemdCgroup = true/' | sed 's/sandbox_image = "registry.k8s.io\/pause:3.6"/sandbox_image = "registry.k8s.io\/pause:3.9"/' | sudo tee /etc/containerd/config.toml

# Restart containerd and check its status

ubuntu@devops-02:~$ sudo systemctl restart containerd
ubuntu@devops-02:~$ sudo systemctl status containerd

# Install kubeadm, kubelet and kubectl

sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl gpg

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.34/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.34/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

# Intialize control plane nodes

sudo kubeadm init --pod-network-cidr=192.168.1.0/24 --apiserver-advertise-address=192.168.1.2

You should see the message ... Your Kubernetes control-plane has initialized successfully!

# To start using your cluster, you need to run the foll as * a REGULAR user*

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Lastly we need to install the CNI plugin
# A CNI plugin is required to implement the Kubernetes network model. This allows pods to communicate with each other and with other network endpoints.
# 1. What is a CNI?

# CNI = Container Network Interface.
# It’s a specification (from the CNCF) plus a set of plugins that handle networking for containers.

# Kubernetes itself doesn’t do container networking.

# Instead, it relies on CNI plugins to implement the “Kubernetes network model.”

# 👉 Think of Kubernetes as saying:
# "Every Pod must get its own IP, and all Pods should be able to talk to each other without NAT. Also, Services need load-balancing across Pods."
# But Kubernetes doesn’t say how — that’s where CNIs come in.

# 🔹 2. Why is it important?

# Without a CNI:

# Pods can be created, but they’ll be stuck in ContainerCreating because there’s no networking.

# Kubelet calls into the CNI binary (like Flannel, Calico, Cilium) to set up veth pairs, bridges, routing, and IPAM.

# CNIs also often handle NetworkPolicies, encapsulation/overlay networks, and sometimes encryption.

# So basically:
# ➡️ No CNI → no working cluster network.
# ➡️ Installing a CNI is mandatory for a functional multi-node Kubernetes cluster.
# 3. Flannel vs Calico

# Two of the most popular CNIs you’ll encounter:

# 🟦 Flannel

# Simple, lightweight CNI.

# Focus: Pod-to-Pod connectivity only (using VXLAN or host-gw).

# Doesn’t support Kubernetes NetworkPolicies.

# Great for small labs, dev/test clusters, or when you just want networking to work.

# 🟧 Calico

# More feature-rich and enterprise-grade.

# Offers:

# Pod-to-Pod connectivity (BGP, VXLAN, IPIP, or native routing)

# Kubernetes NetworkPolicies (fine-grained security between Pods)

# Encryption (WireGuard)

# Scalability to very large clusters.

# Slightly more complex to configure, but widely used in production.

# 🔹 4. Which should you choose?

# For learning / simple clusters:
# 👉 Flannel is easier, one YAML apply, minimal config.

# For production / enterprise:
# 👉 Calico is better because you’ll want NetworkPolicies (for zero-trust, compliance, security) and flexibility in routing modes.

# Performance note:

# Calico in BGP mode (no encapsulation) can be very efficient.

# Flannel always uses an overlay unless you tweak it.